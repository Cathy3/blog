---
title: 随机森林(Random Forest)算法梳理
date: 2019-04-04
categories: 
	-   机器学习
tags:  
        -   sklearn
        -   机器学习
        -   算法
mathjax: true
---

**学习任务：**

-   集成学习
    1.  什么是集成学习？
    2.  集成学习的种类有哪些？
    3.  集成学习的基本步骤：个体学习器的选择、训练、结合
-   bagging算法
-   随机森林思想
-   随机森林的推广
-   sklearn参数
 
<!-- more -->

# 集成学习
集成学习（ensemble learning）是机器学习中的一种思想，而不是指某一具体算法，它通过构建并结合多个学习器来完成学习任务。

集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。

## 集成学习的种类
根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类：

-   个体学习器问存在强依赖关系、必须串行生成的序列化方法，代表是 Boosting；
-   个体学习器间不存在强依赖关系、可同时生成的并行化方法，代表是 Bagging 和"随机森林" (Random Forest)。

Boosting 的基本思路是将基分类器层层叠加， 每一层在训练的时候， 对前一层基分类器中分错的样本， 给予更高的权重。 测试时， 根据各层分类器的结果的加权得到最终结果。（降低偏差）

Bagging 方法更像是一个集体决策的过程， 每个个体都进行单独学习， 学习的内容可以相同， 也可以不同， 也可以部分重叠。 但由于个体之间存在差异性， 最终做出的判断不会完全一致。 在最终做决策时， 每个个体单独作出判断， 再通过投票的方式做出最后的集体决策。 （降低方差）

## 集成学习的基本步骤
集成学习一般可分为以下3个步骤。

1. 找到误差互相独立的个体学习器。
    -   这里可以选取ID3决策树作为基分类器。 事实上， 任何分类模型都可以作为基分类器， 但树形模型由于结构简单且较易产生随机性所以比较常用。
2. 训练个体学习器。
    -   在集成学习中需有效地生成多样性大的个体学习器。与简单地直接用初始数据训练出个体学习器相比，如何增强多样性呢?一般思路是在学习过程中引入随机性，常见做法主要是对数据样本、 输入属性、输出表示 、 算法参数进行扰动。

        -   数据样本扰动：通常是基于采样法， 例如在 Bagging中使用自助采样(即有放回的采样)，在 AdaBoost 中使用序列采样.
        -   输入属性扰动：从初始属性集中抽取出若干个属性子集，再基于每个属性子集训练一个基学习器。比如随机子空间 (random subspace)算法。
        -   输出表示扰动：可对训练样本的类标记稍作变动，如"翻转法" (Flipping Output)随机改变一些训练样本的标记;也可对输出表示进行转化，如"输出调制法" (Output Smearing) 将分类输出转化为回归输出后构建个体学习器;
        -   算法参数扰动：基学习算法一般都有参数需进行设置，例如神经网络的隐层神经元数、初始连接权值等。通过随机设置不同的参数，往往可产生差别较大的个体学习器.
3.  合并个体学习器的结果。
    -   常见的结合策略：平均法(averaging)，投票法(voting)，学习法(比如 stacking)
        -   averaging 常用于数值型输出。有简单平均法、加权平均法等。
        -   voting 是用投票的方式， 将获得最多选票的结果作为最终的结果。常用于分类任务。 
        -   stacking 是用串行的方式， 把前一个基分类器的结果输出到下一个分类器， 将所有基分类器的输出结果相加（或者用更复杂的算法融合） 作为最终的输出。

# Bagging算法

## 采样，训练
在Bootstrap(有放回抽样)的基础上可以构造出Bagging（Bootstrap Aggregating）算法。这种方法对训练样本集进行多次Bootstrap抽样，用每次抽样形成的数据集训练一个弱学习器模型，得到多个独立的弱学习器（对于分类问题，称为弱分类器），最后用它们的组合进行预测。

## 结合策略
在对预测输出进行结合时， Bagging 通常对分类任务使用简单投票法，对回归任务使用简单平均法. 

若分类预测时出现两个类收到同样票数的情形，则最简单的做法是随机选择一个，也可进一步考察学习器投票的置信度来确定最终胜者. 

## 算法特点
-    假定基学习器的计算复杂度为 O(m) ， 则 Bagging 的复杂度大致为$T(O(m) + O (s))$ ，考虑到采样与投票/平均过程的复杂度 $O(s)$ 很小，而$T$通常是一个不太大的常数，因此，训练一个 Bagging 集成与直接使用基学习算法训练一个学习器的复杂度同阶，这说明 Bagging 是一个很高效的集成学习算法. 
-    因为Boostrap采样过程，每个基学习器只使用了初始训练集中约 63.2% 的样本，那么剩下约 36.8% 的样本可用作验证集来对泛化性能进行"包外估计" (out-oιbag estimate) 
-    从偏差方差分解的角度看， Bagging 主要关注降低方差，因此它在不剪枝
决策树、神经网络等易受样本扰动的学习器上效用更为明显.

# 随机森林思想
随机森林(Random Forest，简称RF)是 Bagging 的一个扩展变体. RF 在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机属性选择.  (数据样本扰动 + 输入属性扰动)

-    传统决策树在选择划分属性时是在当前结点的属性集合(假定有 d 个属性)中选择一个最优属性;
-    而在RF 中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含 k 个属性的子集，然后再从这个子集中选择一个最优属性用于划分. 

这里的参数 k 控制了随机性的引入程度：

-   若令 $k = d$ ， 则基决策树的构建与传统决策树相同;
-   若令 $k = 1$ ， 则是随机选择一个属性用于划分 ; 
-   一般情况下，推荐值 $k = log_{2}d$

## 特点
-   随机森林简单、容易实现、计算开销小，令人惊奇的是，它在很多现实任务中展现出强大的性能。
-   随机森林的训练效率常优于 Bagging。
    -   因为在个体决策树的构建过程中，Bagging使用的是 "确定型" 决策树，在选择划分属性时要对结点的所有属性进行考察，而随机森林使用的"随机型"决策树则只需考察一个属性子集。

-   由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。
-   在训练后，可以给出各个特征对于输出的重要性。

缺点

-   在某些噪音比较大的样本集上，RF模型容易陷入过拟合。



# 随机森林的推广
基于RF，有很多变种算法，应用也很广泛，不光可以用于分类回归，还可以用于特征转换，异常点检测等。

# sklearn参数
RF的分类器是RandomForestClassifier，回归器是RandomForestRegressor。RF需要调的参数包括两部分，第一部分是Bagging框架的参数，第二部分是CART决策树的参数。

```python
class sklearn.ensemble.RandomForestClassifier(
        n_estimators=’warn’, criterion=’gini’, 
        max_depth=None, min_samples_split=2, 
        min_samples_leaf=1, 
        min_weight_fraction_leaf=0.0, 
        max_features=’auto’, max_leaf_nodes=None, 
        min_impurity_decrease=0.0, 
        min_impurity_split=None, bootstrap=True, 
        oob_score=False, n_jobs=None, 
        random_state=None, verbose=0, 
        warm_start=False, class_weight=None)
```

## Bagging框架的参数解释

-   **n_estimators**：弱学习器的最大迭代次数，或者说最大的弱学习器的个数。
一般来说n_estimators太小，容易过拟合，太大又容易欠拟合，一般选择一个适中的数值。默认是100.实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。

-   **oob_score**：即是否采用袋外样本来评估模型的好坏。默认设置False。个人推荐设置为True，因为袋外分数反应了一个模型拟合后的泛化能力。

-   **criterion**：即CART决策树做划分时对特征的评价标准。分类模型和回归模型的损失函数不一样。
    -   分类RF对应的CART分类树默认是基尼系数，另一个可选择的标准是信息增益。
    -   回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准时绝对值mae。
    -   一般来说选择默认的标准就已经很好了。

RF重要的框架参数比较少，主要关注的是n_estimators，即RF最大的决策树个数。

## RF决策树参数

RF的决策树参数，它要调参的参数基本和GBDT相同，如下:

-   **max_features**:RF划分时考虑的最大特征，可以使用很多种类型的值.
    -   默认是"auto"，意味着划分时最多考虑 $\sqrt{N}$ 个特征；
    -   如果是"log2"意味着划分时最多考虑 $log_{2}N$ 个特征；
    -   如果是"sqrt"或者"auto"意味着划分时最多可考虑 $\sqrt{N}$ 个特征。
    -   如果是整数，代表考虑的特征绝对数。
    -   如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。
    -   其中N为样本总特征数。
    -   一般我们默认的"auto"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。
-   **max_depth**: 决策树最大深度。默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。
    -   一般来说，数据少或者特征少的时候可以不管这个值。
    -   如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。
-   **min_samples_split**: 内部节点再划分所需最小样本数。这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 
    -   默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。
-   **min_samples_leaf**: 叶子节点最少样本数。这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 
    -   默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。
-   **min_weight_fraction_leaf**：叶子节点最小的样本权重和。这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。
    -   默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。
-   **max_leaf_nodes**: 最大叶子节点数。通过限制最大叶子节点数，可以防止过拟合，默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征很多的话，可以加以限制，具体的值可以通过交叉验证得到。
-   **min_impurity_split**: 节点划分最小不纯度。这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。

上面决策树参数中最重要的包括最大特征数 **max_features**， 最大深度 **max_depth**， 内部节点再划分所需最小样本数**min_samples_split** 和叶子节点最少样本数 **min_samples_leaf**。




# 参考
-    《机器学习》周志华
-    《百面机器学习》诸葛越
-    [随机森林概述 by SIGAI](https://mp.weixin.qq.com/s?src=11&timestamp=1554345104&ver=1525&signature=nv3IJ0jWadpcSy4ZJz6CRoEChzTMau2o9zf3Nl736gN4pWbufSm9XvgY7nh2EykPaJ37oX2iroXw9Ai0sLIQNxmi5r3mtXNDSjypQ*poIZkDqhyy3CrCMD-IB*0XU*8N&new=1)
-    [随机森林算法介绍(理论)](https://zhuanlan.zhihu.com/p/52704839)
-    [scikit-learn随机森林调参小结](https://www.cnblogs.com/pinard/p/6160412.html)