---
title: 模型选择
date: 2018-04-16 
categories: 
        - 机器学习
tags:  
        - 机器学习
        - sklearn
        - 模型选择
        - 交叉验证法
---
# 评估回归模型
这里将都使用波士顿房价的数据集，用10折交叉验证来分离数据，通过均方误差来评估模型性能。
在scikit-learn中用cross_val_score()函数来测试模型，选用负均方误差（neg_mean_squared_error）作为score，得分是负数，这里的neg_mean_squared_error是一种奖励函数，优化的目标是使其最大化。

## 线性算法

### 线性回归
```python
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
#导入数据
data = pd.read_csv('housing.csv')
#划分数据
X = data.values[:,0:13]
Y = data.values[:,13]

seed = 7
kfold = KFold(n_splits=10, random_state = seed)
model = LinearRegression()
scoring = 'neg_mean_squared_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('Linear Regression: %.3f' %result.mean())
```
执行结果如下：
<!-- more -->
```
Linear Regression: -34.705
```

### 岭回归(Ridge Regression)
岭回归是加入L2正则的最小二乘，Sklearn库提供了函数Ridge(alpha)，alpha是超参数，是正则化项的系数，用来弱化变量参数共线性，限制变量权重（参数）过大，alpha越大，越不容易过拟合。
```python
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Ridge
#导入数据
data = pd.read_csv('housing.csv')
#划分数据
X = data.values[:,0:13]
Y = data.values[:,13]

seed = 7
kfold = KFold(n_splits=10, random_state = seed)
model = Ridge()
scoring = 'neg_mean_squared_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('Ridge Regression: %.3f' %result.mean())
```
执行结果如下：
```
Ridge Regression: -34.078
```

### LASSO回归
```python
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Lasso
#导入数据
data = pd.read_csv('housing.csv')
#划分数据
X = data.values[:,0:13]
Y = data.values[:,13]

seed = 7
kfold = KFold(n_splits=10, random_state = seed)
model = Lasso()
scoring = 'neg_mean_squared_error'
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('Lasso Regression: %.3f' %result.mean())
```
执行结果如下：
```
Lasso Regression: -34.464
```

### 弹性网络(Elastic Net)回归
```python
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import ElasticNet
#导入数据
data = pd.read_csv('housing.csv')
#划分数据
X = data.values[:,0:13]
Y = data.values[:,13]

seed = 7
kfold = KFold(n_splits=10, random_state = seed)
scoring = 'neg_mean_squared_error'

model = ElasticNet()
result = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print('ElasticNet Regression: %.3f' % result.mean())
```
执行结果如下：
```
ElasticNet Regression: -31.165
```

## 非线性算法
这里介绍在scikit-learn中的三种非线性的机器学习的回归算法。

-    K近邻（KNN）
-    决策树（CART）
-    支持向量机（SVM）
这三个算法在分类算法中同样存在，代码类似于上面的算法，这里就写在一起。
```python
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
#导入数据
data = pd.read_csv('housing.csv')
#划分数据
X = data.values[:,0:13]
Y = data.values[:,13]

seed = 7
kfold = KFold(n_splits=10, random_state = seed)
scoring = 'neg_mean_squared_error'

models = {}
models['KNN'] = KNeighborsRegressor()
models['DecisionTree'] = DecisionTreeRegressor()
models['SVM'] = SVR()

results = []
for name in models:
    result = cross_val_score(models[name], X, Y, cv=kfold, scoring=scoring)
    results.append(result)
    print('%s: %.3f' % (name, result.mean()))
```
执行结果如下：
```
KNN: -107.287
DecisionTree: -44.421
SVM: -91.048
```

# 评估分类模型
这里会介绍6种分类算法：
线性算法：

-    逻辑回归
-    线性判别（LDA）

非线性算法：

-    K近邻（KNN）
-    贝叶斯分类器
-    决策树
-    支持向量机（SVM）

```python
import pandas as pd
from matplotlib import pyplot

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

#导入数据
data = pd.read_csv('pima_data.csv')
#划分数据
X = data.values[:,0:8]
Y = data.values[:,8]

seed = 7
kfold = KFold(n_splits=10, random_state = seed)

models = {}
models['LR'] = LogisticRegression()
models['LDA'] = LinearDiscriminantAnalysis()
models['KNN'] = KNeighborsClassifier()
models['CART'] = DecisionTreeClassifier()
models['SVM'] = SVC()
models['NB'] = GaussianNB()

results = []
for name in models:
    result = cross_val_score(models[name], X, Y, cv=kfold)
    results.append(result)
    print('%s: %.3f(%.3f)' % (name, result.mean(), result.std()))

# 图表显示
fig = pyplot.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(models.keys())
pyplot.show()
```
执行结果如下：
```
LR: 0.770(0.048)
LDA: 0.773(0.052)
KNN: 0.727(0.062)
CART: 0.690(0.062)
SVM: 0.651(0.072)
NB: 0.755(0.043)
```
![图表显示](http://wx3.sinaimg.cn/large/9b7d0c6fly1fqia3kgx3nj20al07p3yh.jpg "title")


# 参考：
1.[《机器学习之python》](https://read.douban.com/reader/column/6939417/chapter/35955529/)