---
title: Logistic回归
date: 2018-01-23 15:27:33
tags:
---
Logistic回归是统计学习中的经典分类方法。其实仅在线性回归的基础上，套用了一个逻辑函数。本文主要详述逻辑回归模型的基础。

假设有输入向量 $x=(x_1;x_2;...;x_n)$， 由线性回归模型产生的预测值 
<!-- more -->
$$z=w ^ \mathrm{ T }x+b\tag{1}$$ 是实值，需要转换为 0/1 值才能进行分类。因此找一个单调可微函数来替代，将$z$ 的值映射到 (0,1) 之间，这里采用logistic函数：
$$y=\frac{1}{1+e^{-z}}\tag{2}$$
将式(1)代入得到             $$y=\frac{1}{1+e^{-(w ^ \mathrm{ T }x+b)}}\tag{3}$$                         上式可变化为对数几率形式：
$$\ln \frac{y}{1-y}=w ^ \mathrm{ T }x+b\tag{4}$$
由此看出，真实标记 $y$ 的对数几率是输入 $x$ 的线性函数。
将 $y$ 视为类后验概率估计 $p(y=1|x)$ ，则上式可重写为
$$\ln \frac{p(y=1|x)}{p(y=0|x)}=w ^ \mathrm{ T }x+b\tag{5}$$
显然有
$$p(y=1|x)=\frac{e^{w ^ \mathrm{ T }x+b}}{1+e^{w ^ \mathrm{ T }x+b}}\tag{6}$$
$$p(y=0|x)=\frac{1}{1+e^{w ^ \mathrm{ T }x+b}}\tag{7}$$
此时线性函数的值越接近正无穷，概率值就越接近1；线性函数的值越接近负无穷，概率值就越接近0。这样的模型就是logistic回归模型。

接下来可通过“极大似然法”来估计 $w$ 和 $b$ 。
