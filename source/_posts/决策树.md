---
title: 决策树(Decision trees)
date: 2019-03-18
categories: 
	-   机器学习
tags:  
        -   python
        -   机器学习
mathjax: true
---

决策树模型核心是下面几部分：

-    结点和有向边组成
-    结点有内部结点和叶结点俩种类型
-    内部结点表示一个特征，叶节点表示一个类

决策树的判定过程就相当于树中从根结点到某一个叶子结点的遍历。每一步如何遍历是由数据各个特征的具体特征属性决定。

<!-- more -->

# 构建决策树
所谓决策树的构造，就是进行属性选择度量，确定各个特征属性之间的拓扑结构。

属性选择度量是一种选择分裂准则，是将给定的类标记的训练集合的数据划分D“最好”地分成个体类的启发式方法，它决定了拓扑结构及分裂点split_point的选择。

属性选择度量算法有很多，一般使用自顶向下递归分治法，并采用不回溯的贪心策略。

## 分裂属性

构造决策树的关键步骤是分裂属性。

分裂属性分为三种不同的情况：

1.   属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。
2.   属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。
3.   属性是连续值。此时确定一个值作为分裂点split_point，按照>split_point和<=split_point生成两个分支。



分裂属性：在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。

判断“纯”的方法不同引出了我们的ID3算法，C4.5算法以及CART算法。


# ID3算法
在信息论中，期望信息越小，信息增益越大，从而纯度越高。

ID3算法的核心思想就是以信息增益来度量属性选择，选择分裂后信息增益最大的属性进行分裂。

下面先定义几个要用到的概念。

设 D 为用类别对训练元组进行的划分，则 D 的熵（entropy）表示为：

$$
i n f o(D)=-\sum_{i=1}^{m} p_{i} \log _{2}\left(p_{i}\right)
$$

其中 $p_i$ 表示第 $i$ 个类别在整个训练元组中出现的概率，可以用$\frac{属于此类别元素的数量} {训练元组元素总数量}$ 作为估计。熵的实际意义表示是 D 中元组的类标号所需要的平均信息量。

现在我们假设将训练元组 D 按属性 A 进行划分，则 A 对 D 划分的期望信息为：

$$
i n f o_{A}(D)=\sum_{j=1}^{v} \frac{\left|D_{j}\right|}{|D|} \text i n f o\left(D_{j}\right)
$$

 而信息增益即为两者的差值：

$$
\operatorname{gain}(A)=i n f o(D)-i n f o_{A}(D)
$$

ID3算法就是在每次需要分裂时，计算每个属性的增益率，然后选择增益率最大的属性进行分裂。




# C4.5算法
ID3算法存在一个问题，就是偏向于多值属性，例如，如果存在唯一标识属性ID，则ID3会选择它作为分裂属性，这样虽然使得划分充分纯净，但这种划分对分类几乎毫无用处。ID3的后继算法C4.5使用增益率（gain ratio）的信息增益扩充，试图克服这个偏倚。

# CART算法
CART(Classification And Regression Tree)分类回归树算法采用一种二分递归分割的技术，将当前的样本集分为两个子样本集，使得生成的url子节点都有两个分支。因此，CART算法生成的决策树是结构简洁的二叉树。

由于CART算法构成的是一个二叉树，它在每一步的决策时只能是“是”或者“否”，即使一个feature有多个取值，也是把数据分为两部分。在CART算法中主要分为两个步骤:

1.   将样本递归划分进行建树过程
2.   用验证数据进行剪枝

## 划分建树
设 $x_{1}, x_{2}, \dots, x_{n}$ 代表单个样本的 $n$ 个属性，$y$ 表示所属类别。CART算法通过递归的方式将维的空间划分为不重叠的矩形。划分步骤大致如下:

1.   选一个自变量 $x_i$，再选取 $x_i$ 的一个值 $v_i$， $v_i$把 n 维空间划分为两部分，一部分的所有点都满足 $x_{i} \leq v_{i}$，另一部分的所有点都满足 $x_{i} > v_{i}$，对非连续变量来说属性值的取值只有两个，即等于该值或不等于该值。
2.   递归处理，将上面得到的两部分按步骤1 重新选取一个属性继续划分，直到把整个维空间都划分完。

在划分时候有一个问题，它是按照什么标准来划分的 ？ 

对于一个变量属性来说，它的划分点是一对连续变量属性值的中点。假设 m 个样本的集合一个属性有 m 个连续的值，那么则会有 m-1 个分裂点，每个分裂点为相邻两个连续值的均值。每个属性的划分按照能减少的杂质的量来进行排序，而杂质的减少量定义为划分前的杂质减去划分后的每个节点的杂质量划分所占比率之和。而杂质度量方法常用**Gini指标**，假设一个样本共有 $C$ 类，那么一个节点 $A$ 的Gini不纯度可定义为

$$
\operatorname{Gini}(A)=1-\sum_{i=1}^{C} p_{i}^{2}
$$


其中 $p_i$ 表示属于 $i$ 类的概率，当 Gini(A)=0 时，所有样本属于同类，所有类在节点中以等概率出现时，Gini(A) 最大化，此时 $C(C-1)/2$ 。

 
有了上述理论基础，实际的递归划分过程是这样的：如果当前节点的所有样本都不属于同一类或者只剩下一个样本，那么此节点为非叶子节点，所以会尝试样本的每个属性以及每个属性对应的分裂点，尝试找到杂质变量最大的一个划分，该属性划分的子树即为最优分支。

## 剪枝

建树完成后，可以根据验证数据进行剪枝。在CART树的建树过程中，可能存在Overfitting，许多分支中反映的是数据中的异常，这样的决策树对分类的准确性不高，那么需要检测并减去这些不可靠的分支。

决策树常用的剪枝有事前剪枝和事后剪枝，CART算法采用事后剪枝，具体方法为代价复杂性剪枝法。可参考如下链接

   剪枝参考：http://www.cnblogs.com/zhangchaoyang/articles/2709922.html

# 参考
-    [算法杂货铺——分类算法之决策树(Decision tree)](http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html)
-    [决策树之CART算法](https://blog.csdn.net/acdreamers/article/details/44664481)